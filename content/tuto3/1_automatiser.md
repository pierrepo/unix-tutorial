# Automatiser l'analyse RNA-seq avec un cluster üöÄ

```{contents}
```

## D√©couvrir le cluster

Un cluster de calcul met √† disposition de ses utilisateurs des processeurs et de la m√©moire vive pour r√©aliser des calculs. Ces ressources sont r√©parties sur plusieurs machines, appel√©es *n≈ìuds de calcul*. Chaque n≈ìud de calcul contient plusieurs dizaines de processeurs (ou c≈ìurs) et plusieurs centaines de Go de m√©moire vive.

Plusieurs utilisateurs utilisent simultan√©ment un cluster de calcul. Pour vous en rendre compte, dans JupyterLab, ouvrez un terminal puis entrez la commande suivante qui va lister tous les calculs (appel√©s *job*) en cours sur le cluster :

```bash
$ squeue -t RUNNING
```

```{admonition} Rappel
:class: tip
Ne tapez pas le caract√®re `$` en d√©but de ligne et faites attention aux majuscules et au minuscules.
```

Vous voyez que vous n'√™tes pas seul ! Comptez maintenant le nombre de jobs en cours d'ex√©cution en cha√Ænant la commande pr√©c√©dente avec `wc -l` :

```bash
$ squeue -t RUNNING | wc -l
```

Vous pouvez aussi compter le nombre de jobs en attente d'ex√©cution :

```bash
$ squeue -t PENDING | wc -l
```

Et vous alors ? Avez-vous lanc√© un calcul ? V√©rifiez-le en entrant la commande suivante pour lister les calculs associ√©s √† votre compte :

```bash
$ squeue -u $USER
```

La colonne `ST` indique le statut de votre job :

- `CA` (*cancelled*) : le job a √©t√© annul√©
- `F` (*failed*) : le job a plant√©
- `PD` (*pending*) : le job est en attente que des ressources soient disponibles
- `R` (*running*) : le job est en cours d'ex√©cution

Bizarre ! Vous avez un job avec le statut *running* en cours d'ex√©cution alors que vous n'avez a priori rien lanc√© ü§î

En fait, le JupyterLab dans lequel vous √™tes est lui-m√™me un job lanc√© sur le cluster. C'est d'ailleurs pour cela qu'avant de lancer JupyterLab, vous avez d√ª pr√©ciser le compte √† utiliser (`2501_duo`) et choisir le nombre de processeurs et la quantit√© de m√©moire vive dont vous aviez besoin. Finalement, vous √©tiez dans la matrice sans m√™me le savoir üò±.

Comme plusieurs utilisateurs peuvent lancer des jobs sur un cluster, un gestionnaire de jobs (ou ordonnanceur) s'occupe de r√©partir les ressources entre les diff√©rents utilisateurs. Sur le cluster de l'IFB, le gestionnaire de jobs est [Slurm](https://slurm.schedmd.com/). D√©sormais le lancement de vos analyses se fera via Slurm.


## Analyser un √©chantillon

D√©placez-vous le r√©pertoire `/shared/projects/2501_duo/$USER/rnaseq` puis v√©rifiez que vous √™tes dans le bon r√©pertoire avec la commande `pwd`.

Supprimez les r√©pertoires qui contiennent les r√©sultats d'une √©ventuelle pr√©c√©dente analyse :

```bash
$ rm -rf genome_index reads_qc reads_map counts
```

T√©l√©chargez dans un terminal de JupyterLab un premier script Bash ([`script_cluster_0.sh`](script_cluster_0.sh)) avec la commande `wget` :

```bash
$ wget https://raw.githubusercontent.com/pierrepo/unix-tutorial/master/content/tuto3/script_cluster_0.sh
```

Ce script correspond au script `script_local_2.sh` adapt√© pour une utilisation sur un cluster. Ouvrez le script `script_cluster_0.sh` avec l'√©diteur de texte de JupyterLab et essayez d'identifier les diff√©rences avec `script_local_2.sh`. 

1. Au d√©but du script, deux lignes sont nouvelles :

    ```
    #SBATCH --mem=2G
    #SBATCH --cpus-per-task=8
    ```

    Ces lignes commencent par le caract√®re `#` qui indique qu'il s'agit de commentaires pour Bash, elles seront donc ignor√©es par le *shell*. Par contre, elles ont un sens tr√®s particulier pour le gestionnaire de jobs du cluster Slurm. Ici, ces lignes indiquent √† Slurm que le job a besoin de 2 Go de m√©moire vive et de 8 processeurs pour s'ex√©cuter.

1. Un peu plus loin, on indique explicitement les modules (les logiciels) √† charger avec leurs versions :

    ```bash
    module load fastqc/0.11.9
    module load star/2.7.10b
    module load samtools/1.15.1
    module load htseq/0.13.5
    module load cufflinks/2.2.1
    ```

1. L'appel aux diff√©rents programmes (`STAR`, `fastqc`, `samtools`...) est pr√©fix√© par l'instruction `srun` qui va explicitement indiquer √† Slurm qu'il s'agit d'un sous-job. Nous en verrons l'utilit√© plus tard.

1. Pour STAR, l'indication du nombre de c≈ìurs √† utiliser est d√©fini sous la forme :

    ```bash
    srun STAR --runThreadN "${SLURM_CPUS_PER_TASK}" \
    ```

    La variable `SLURM_CPUS_PER_TASK` est utilis√©e pour indiquer √† STAR le nombre de processeurs √† utiliser. Cette variable est automatiquement d√©finie par Slurm lors de l'ex√©cution du script et la lecture de l'instruction `#SBATCH --cpus-per-task=8`. Dans le cas pr√©sent, elle vaut 8.


Lancez enfin le script avec la commande suivante :

```bash
$ sbatch -A 2501_duo script_cluster_0.sh
```

Vous devriez obtenir un message du type `Submitted batch job 33389786`. Ici, `33389786` correspond au num√©ro du job. Notez bien le num√©ro de votre job.


- L'instruction `sbatch` est la mani√®re de demander √† Slurm de lancer un script.
- L'option `-A 2501_duo` sp√©cifie quel projet utiliser (facturer) pour cette commande. Un m√™me utilisateur peut appartenir √† plusieurs projets. Le nombre d'heures de calcul attribu√©es √† un projet √©tant limit√©, il est important de savoir quel projet imputer pour telle ou telle commande. Pensez-y pour vos futurs projets.

V√©rifiez que votre script est en train de tourner avec la commande :

```bash
$ squeue -u $USER
```

Le statut du job devrait √™tre *RUNNING* (`R`).

Et pour avoir plus de d√©tails, utilisez la commande :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```

avec `JOBID` (en fin de ligne) le num√©ro du job √† remplacer par le v√¥tre.

Voici un exemple de sortie que vous pourriez obtenir :

```
$  sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 33389786
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
33389786     script_cl+    RUNNING 2023-05-12T10:25:21   00:00:46   00:06:08     cpu-node-11 
33389786.ba+      batch    RUNNING 2023-05-12T10:25:21   00:00:46   00:06:08     cpu-node-11 
33389786.0         STAR  COMPLETED 2023-05-12T10:25:23   00:00:08   00:01:04     cpu-node-11 
33389786.1       fastqc    RUNNING 2023-05-12T10:25:31   00:00:36   00:04:48     cpu-node-11 
```

Quand la colonne *State* est √† `COMPLETED`, cela signifie que le sous-job est termin√©. Un sous-job est cr√©√© √† chaque fois que la commande `srun` est utilis√©e dans le script de soumission. Ici, on voit que le sous-job `STAR` est termin√©, mais que le sous-job `fastqc` est toujours en cours d'ex√©cution.

Relancez r√©guli√®rement la commande pr√©c√©dente pour suivre l'avancement de votre job.

```{note}
Pour rappeller une commande pr√©c√©dente, vous pouvez utiliser la fl√®che du haut de votre clavier. Cela vous √©vitera de retaper la commande √† chaque fois.
```

Notez le moment o√π vous avez termin√© l'alignement sur le g√©nome de r√©f√©rence, c'est-√†-dire que vous avez obtenu un second sous-job `STAR` avec le statut `COMPLETED` dans la sortie renvoy√©e par `sacct`. Par exemple :

```
$  sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j 33389786
       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList 
------------ ---------- ---------- ------------------- ---------- ---------- --------------- 
33389786     script_cl+    RUNNING 2023-05-12T10:25:21   00:03:25   00:27:20     cpu-node-11 
33389786.ba+      batch    RUNNING 2023-05-12T10:25:21   00:03:25   00:27:20     cpu-node-11 
33389786.0         STAR  COMPLETED 2023-05-12T10:25:23   00:00:08   00:01:04     cpu-node-11 
33389786.1       fastqc  COMPLETED 2023-05-12T10:25:31   00:01:23   00:11:04     cpu-node-11 
33389786.2         STAR  COMPLETED 2023-05-12T10:26:54   00:01:36   00:12:48     cpu-node-11 
33389786.3     samtools    RUNNING 2023-05-12T10:28:30   00:00:16   00:02:08     cpu-node-11 
```

Nous ne souhaitons pas poursuivre plus longtemps cette analyse, car cela prendrait trop de temps.

Stoppez le job en cours avec la commande :

```bash
$ scancel JOBID
```

avec `JOBID` le num√©ro de votre job.

V√©rifiez avec la commande `squeue -u $USER` que votre job ne tourne plus.


```{note}
Le fichier `slurm-JOBID.out` (avec `JOBID` le num√©ro de votre job) contient le ¬´ journal ¬ª de votre job. C'est-√†-dire tout ce qui s'affiche habituellement √† l'√©cran a √©t√© enregistr√© dans ce fichier.

Ouvrez ce fichier avec l'√©diteur de texte de JupyterLab pour vous en rendre compte.
```

## Analyser 50 √©chantillons (ou pas loin)

Maintenant que nous savons comment analyser un √©chantillon avec un cluster de calcul, nous allons automatiser l'analyse de 50 √©chantillons. Mais pour cela nous allons prendre plusieurs pr√©cautions :

- Nous allons s√©parer les √©tapes suivantes :
    1. Indexer le g√©nome de r√©f√©rence -- ne doit √™tre fait qu'une seule fois.
    2. Contr√¥ler la qualit√©, aligner et quantifier les *reads* -- doit √™tre fait pour chaque √©chantillon, donc 50 fois.
    3. Normaliser les comptages de tous les √©chantillons ensemble -- ne doit √™tre fait qu'une seule fois.
- L'√©tape 2 ne doit pas se faire successivement pour chaque √©chantillon, mais en parall√®le. C'est-√†-dire qu'on souhaite id√©alement lancer l'analyse des 50 √©chantillons **en m√™me temps**.

### Pr√©parer les donn√©es

Nous avons t√©l√©charg√© pour vous les 50 √©chantillons (fichiers *.fastq.gz*) ainsi que le g√©nome de r√©f√©rence et ses annotations dans le r√©pertoire `/shared/projects/202304_duo/data/rnaseq_scere`. V√©rifiez son contenu avec la commande :

```bash
$ tree /shared/projects/2501_duo/data/rnaseq_scere
```

### Lancer le script de l'√©tape 1 (indexer le g√©nome de r√©f√©rence)

Supprimez les r√©pertoires qui contiennent les r√©sultats d'une √©ventuelle pr√©c√©dente analyse :

```bash
$ rm -rf genome_index reads_qc reads_map counts slurm*.out
```

T√©l√©chargez dans un terminal de JupyterLab le script Bash ([`script_cluster_1.sh`](script_cluster_1.sh)) avec la commande `wget` :

```bash
$ wget https://raw.githubusercontent.com/pierrepo/unix-tutorial/master/content/tuto3/script_cluster_1.sh
```
Ouvrez ce script avec l'√©diteur de texte de JupyterLab et essayez d'identifier comment sont d√©finies les variables `base_dir` et `data_dir`.

Puis lancez le script en n'oubliant pas d'indiquer explicitement le compte √† utiliser (`202304_duo`) :

```bash
$ sbatch -A 2501_duo script_cluster_1.sh
```

V√©rifiez avec les commandes `squeue` et `sacct` que le job est lanc√© et se termine correctement (normalement rapidement).

```{rappel}
Le fichier `slurm-JOBID.out` (avec `JOBID` le num√©ro de votre job) contient le ¬´ journal ¬ª de votre job. N'h√©sitez pas √† le consulter.
```


### Lancer le script de l'√©tape 2 (traiter les √©chantillons)

T√©l√©chargez dans un terminal de JupyterLab le script Bash ([`script_cluster_2.sh`](script_cluster_2.sh)) avec la commande `wget` :

```bash
$ wget https://raw.githubusercontent.com/pierrepo/unix-tutorial/master/content/tuto3/script_cluster_2.sh
```
Ouvrez ce script avec l'√©diteur de texte de JupyterLab. Notez en d√©but de script la ligne :

```
#SBATCH --array=0-49 
```

qui nous permettra de cr√©er un *job array*, c'est-√†-dire un ensemble de 50 (de 0 √† 49 inclus) jobs qui vont √™tre lanc√©s en parall√®le.

Pour chacun des jobs, on lui attribue un √©chantillon avec les commandes suivantes :

```bash
# liste de tous les fichiers .fastq.gz dans un tableau
fastq_files=(${fastq_dir}/*fastq.gz)
# extraction de l'identifiant de l'√©chantillon
# √† partir du nom de fichier : /shared/projects/2501_duo/data/rnaseq_scere/reads/SRR3405783.fastq.gz
# on extrait : SRR3405783
sample=$(basename -s .fastq.gz "${fastq_files[$SLURM_ARRAY_TASK_ID]}")
```

Cette √©tape est importante, car elle permet de savoir quel √©chantillon traiter pour chaque job. Par exemple, le job 0 va √™tre associ√© √† l'√©chantillon `SRR3405783` (le premier par ordre alphab√©tique).

Pour ne pas emboliser le cluster et pour que tout le monde puisse obtenir des r√©sultats rapidement, modifiez la ligne 

```
#SBATCH --array=0-49 
```

par

```
#SBATCH --array=0-2
```

Vous n'analyserez que 3 √©chantillons dans 3 jobs en parall√®le. Pensez √† sauvegarder vos modifications.

Lancez enfin le script :

```bash
$ sbatch -A 2501_duo script_cluster_2.sh
```

Suivez la progression de vos jobs avec la commande :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```

avec `JOBID` le num√©ro de votre job.

Plut√¥t que de relancer en permanence la commande `sacct` vous pouvez demander √† la commande `watch` d'afficher les √©ventuels changements :

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```

Vous devriez obtenir un affichage du type :

```
Every 2.0s: sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j ...  Wed May 10 22:22:53 2023

       JobID    JobName      State               Start    Elapsed    CPUTime        NodeList
------------ ---------- ---------- ------------------- ---------- ---------- ---------------
33361021_0   script_cl+    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-20
33361021_0.+      batch    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-20
33361021_0.0     fastqc    RUNNING 2023-05-10T22:21:56   00:00:57   00:07:36     cpu-node-20
33361021_1   script_cl+    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-23
33361021_1.+      batch    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-23
33361021_1.0     fastqc    RUNNING 2023-05-10T22:21:56   00:00:57   00:07:36     cpu-node-23
33361021_2   script_cl+    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-25
33361021_2.+      batch    RUNNING 2023-05-10T22:21:54   00:00:59   00:07:52     cpu-node-25
33361021_2.0     fastqc    RUNNING 2023-05-10T22:21:56   00:00:57   00:07:36     cpu-node-25
```

On apprend ici que 3 jobs sont en cours d'ex√©cution (`RUNNING`) et qu'ils sont appel√©s `33361021_0`, `33361021_1` et `33361021_2`. Chacun de ces jobs a √©t√© lanc√© sur un noeud de calcul diff√©rent (`cpu-node-20`, `cpu-node-23` et `cpu-node-25`) mais cela aurait pu √™tre le m√™me. Chaque job est d√©compos√© en sous-jobs, pour le moment √† l'√©tape `fastqc`.

Le temps que les 3 jobs se terminent, profitez-en pour faire une pause caf√© ‚òïÔ∏è bien m√©rit√©e et r√©aliser √† quel point la bioinformatique et ses possibilit√©s d'automatisation sont cools.

```{hint}
Utilisez la combinaison de touches <kbd>Ctrl</kbd> + <kbd>C</kbd> pour arr√™ter la commande `watch`.
```

Quand tous les jobs sont √† `COMPLETED`, comparez le temps d'ex√©cution de chacun (dans la colonne *Elapsed* au niveau des lignes *script_cl+*). Ce temps d'ex√©cution devrait √™tre compris entre 15 et 20 min, ce qui est √©quivalent au temps d'ex√©cution du script `script_local_2.sh` pour un **seul √©chantillon**. C'est tout l'int√©r√™t de lancer des jobs en parall√®le.

Remarquez que le temps CPU (`CPUTime`) qui correspond au temps de calcul consomm√© par tous les processeurs est sup√©rieur au temps ¬´ humain ¬ª (*Elapsed*). Ainsi, si vous avez lanc√© un job avec 8 coeurs qui se termine en 1 heure, la consommation CPU sera de 8 coeurs x 1 heure = 8 heures. Sur un cluster, c'est toujours le temps CPU qui est factur√©.


### Lancer le script de l'√©tape 3 (normaliser les comptages)

Vous allez maintenant normaliser les diff√©rents comptages.

T√©l√©chargez dans un terminal de JupyterLab le script Bash ([`script_cluster_3.sh`](script_cluster_3.sh)) avec la commande `wget` :

```bash
$ wget https://raw.githubusercontent.com/pierrepo/unix-tutorial/master/content/tuto3/script_cluster_3.sh
```

Ouvrez ce script avec l'√©diteur de texte de JupyterLab. Retrouvez les lignes sp√©cifiques √† l'utilisation d'un cluster de calcul et qui d√©butent par :

- `#SBATCH`
- `module load`
- `srun`

Puis lancez le script :

```bash
$ sbatch -A 2501_duo script_cluster_3.sh
```

Affichez l'avancement de votre job avec la commande `sacct` :

```bash
$ sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```

ou

```bash
$ watch sacct --format=JobID,JobName,State,Start,Elapsed,CPUTime,NodeList -j JOBID
```

avec `JOBID` le num√©ro de votre job.

L'ex√©cution de ce script devrait moins d'une minute.

Le fichier qui contient le comptage normalis√© des transcrits est `counts/genes.count_table`.


## Bilan

Vous avez lanc√© une analyse RNA-seq compl√®te en utilisant le gestionnaire de ressources (Slurm) d'un cluster de calcul. Bravo ‚ú®

Un peu plus tard, nous vous inviterons √† reprendre cette analyse, mais cette fois sur les 50 √©chantillons. Pour cela, il vous faudra modifier le script `script_cluster_2.sh` en rempla√ßant la ligne `#SBATCH --array=0-2` (pour 3 √©chantillons) par `#SBATCH --array=0-49` (pour 50 √©chantillons). Pensez aussi √† relancer le script `script_cluster_3.sh` pour normaliser les r√©sultats de comptage.


## Aller plus loin : connecter les jobs

```{note}
Vous n'√™tes pas oblig√© de lancer les commandes ci-dessous. Elles sont l√† pour vous montrer comment automatiser le lancement de plusieurs jobs les uns apr√®s les autres.
```

Pour cette analyse, il faut lancer 3 scripts √† la suite : `script_cluster_1.sh`, `script_cluster_2.sh` et `script_cluster_3.sh`. √Ä chaque fois, il faut attendre que le pr√©c√©dent soit termin√©, ce qui peut √™tre p√©nible. Slurm offre la possibilit√© de cha√Æner les jobs les uns avec les autres avec l'option `--dependency`. Voici un exemple d'utilisation.

Tout d'abord on lance le script `script_cluster_1.sh` :

```bash
$ sbatch -A 2501_duo script_cluster_1.sh
Submitted batch job 33390286
```

On r√©cup√®re le job id (`33390286`) puis on lance imm√©diatement le script `script_cluster_2.sh` :
```bash
$ sbatch -A 2501_duo --dependency=afterok:33390286 script_cluster_2.sh
Submitted batch job 33390299
```

On r√©cup√®re le nouveau job id (`33390299`) puis on lance imm√©diatement le dernier script `script_cluster_3.sh` :

```bash
$ sbatch -A 2501_duo --dependency=afterok:33390299 script_cluster_3.sh
Submitted batch job 33390315
```

Dans l'exemple ci-dessus, le job `33390315` (pour `script_cluster_3.sh`) ne va s'ex√©cuter que quand le job `33390299` (pour `script_cluster_2.sh`) sera termin√© avec succ√®s. Et le job `33390299` ne va s'ex√©cuter que quand le job `33390286` (pour `script_cluster_1.sh`) sera, lui aussi, termin√© avec succ√®s.

Voici le r√©sultat obtenu avec `squeue` : 

```
$ squeue -u ppoulain
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          33390315      fast script_c ppoulain PD       0:00      1 (Dependency)
          33389748      fast  jupyter ppoulain  R      28:36      1 cpu-node-15
        33390299_0      fast script_c ppoulain  R       1:44      1 cpu-node-2
        33390299_1      fast script_c ppoulain  R       1:44      1 cpu-node-30
        33390299_2      fast script_c ppoulain  R       1:44      1 cpu-node-35
```

Dans cet exemple le premier job (`33390286`) est d√©j√† termin√© (et n'apparait pas). Le job `33390299` est en cours d'ex√©cution (pour 3 √©chantillons seulement) et le job `33390315` est en attente que le job `33390299` se termine.

Cette m√©thode √©vite d'attendre que le job pr√©c√©dent se termine pour lancer le suivant, mais il faut quand m√™me les lancer manuellement pour r√©cup√©rer les job ids. On peut automatiser cela avec les commandes suivantes :


```bash
jobid1=$(sbatch -A 2501_duo script_cluster_1.sh | cut -d' ' -f 4)
echo "Running job 1: ${jobid1}"
jobid2=$(sbatch -A 2501_duo --dependency=afterok:${jobid1} script_cluster_2.sh | cut -d' ' -f 4)
echo "Running job 2: ${jobid2}"
sbatch -A 2501_duo --dependency=afterok:${jobid2} script_cluster_3.sh
squeue -u $USER
```

La commande `squeue` √† la fin permet simplement de v√©rifier que le premier job est en cours d'ex√©cution et que les deux autres sont en attente.
